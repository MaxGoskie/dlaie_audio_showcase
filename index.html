<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>dlaie_blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">dlaie_blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#adding-guidance-based-stylistic-controls-to-flow-models-for-coherent-audio-generation" id="toc-adding-guidance-based-stylistic-controls-to-flow-models-for-coherent-audio-generation" class="nav-link active" data-scroll-target="#adding-guidance-based-stylistic-controls-to-flow-models-for-coherent-audio-generation">Adding Guidance-Based Stylistic Controls to Flow Models for Coherent Audio Generation</a>
  <ul class="collapse">
  <li><a href="#abstract" id="toc-abstract" class="nav-link" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#project-overview" id="toc-project-overview" class="nav-link" data-scroll-target="#project-overview">Project Overview</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a>
  <ul class="collapse">
  <li><a href="#what-is-a-flow-model" id="toc-what-is-a-flow-model" class="nav-link" data-scroll-target="#what-is-a-flow-model">What is a Flow Model?</a></li>
  <li><a href="#what-is-guidance" id="toc-what-is-guidance" class="nav-link" data-scroll-target="#what-is-guidance">What is Guidance?</a></li>
  </ul></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology</a>
  <ul class="collapse">
  <li><a href="#continuous-pitch" id="toc-continuous-pitch" class="nav-link" data-scroll-target="#continuous-pitch">Continuous Pitch</a></li>
  <li><a href="#discrete-pitch" id="toc-discrete-pitch" class="nav-link" data-scroll-target="#discrete-pitch">Discrete Pitch</a></li>
  <li><a href="#beat" id="toc-beat" class="nav-link" data-scroll-target="#beat">Beat</a></li>
  </ul></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a>
  <ul class="collapse">
  <li><a href="#general-limitations" id="toc-general-limitations" class="nav-link" data-scroll-target="#general-limitations">General Limitations</a></li>
  <li><a href="#continuous-pitch-limitations" id="toc-continuous-pitch-limitations" class="nav-link" data-scroll-target="#continuous-pitch-limitations">Continuous Pitch Limitations</a></li>
  <li><a href="#discrete-pitch-limitations" id="toc-discrete-pitch-limitations" class="nav-link" data-scroll-target="#discrete-pitch-limitations">Discrete Pitch Limitations</a></li>
  <li><a href="#beat-limitations" id="toc-beat-limitations" class="nav-link" data-scroll-target="#beat-limitations">Beat Limitations</a></li>
  </ul></li>
  <li><a href="#an-educational-approach" id="toc-an-educational-approach" class="nav-link" data-scroll-target="#an-educational-approach">An Educational Approach</a>
  <ul class="collapse">
  <li><a href="#deep-learning-briefer" id="toc-deep-learning-briefer" class="nav-link" data-scroll-target="#deep-learning-briefer">Deep Learning Briefer</a></li>
  <li><a href="#getting-up-to-speed" id="toc-getting-up-to-speed" class="nav-link" data-scroll-target="#getting-up-to-speed">Getting Up to Speed</a></li>
  </ul></li>
  <li><a href="#conclusion-and-discussion" id="toc-conclusion-and-discussion" class="nav-link" data-scroll-target="#conclusion-and-discussion">Conclusion and Discussion</a></li>
  <li><a href="#poster-presented-at-belmont-universitys-surs-2025" id="toc-poster-presented-at-belmont-universitys-surs-2025" class="nav-link" data-scroll-target="#poster-presented-at-belmont-universitys-surs-2025">Poster Presented at Belmont University’s SURS 2025</a></li>
  <li><a href="#acknowledgement" id="toc-acknowledgement" class="nav-link" data-scroll-target="#acknowledgement">Acknowledgement</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="adding-guidance-based-stylistic-controls-to-flow-models-for-coherent-audio-generation" class="level1">
<h1>Adding Guidance-Based Stylistic Controls to Flow Models for Coherent Audio Generation</h1>
<p><strong>Authors:</strong> Zain Alsaad, Simeon Betapudi, Brody Blackwood, Marco Cassar, Kenneth Chau, Jayden Cruz, Evan Dant, Kritan Duwal, Rush Gorney, Maxwell Goskie, Elijah Konkle, Hari Patel, Mayur Patel, Brady Pinter, and Christopher White</p>
<p><strong>Advisor:</strong> Scott Hawley, Ph.D.</p>
<p><em>Belmont University, Nashville, TN, USA</em></p>
<hr>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>We present an accessible approach to controllable audio generation that leverages guidance-based flow models to create audio. These models operate by learning latent representations of audio and decoding them. A project-based course was implemented where students explored generative audio using Stable Audio Open. Using chords, melody, and beat as core guidance mechanisms, a custom guidance model was built using mix tracks from Belmont University’s audio archives to steer generation toward higher stylistic coherence. Beyond technical outcomes, this approach highlights the value of interactive, hands-on learning in audio AI education where students learn by deeply engaging with model training, evaluation, and creative iteration. However, the results were ultimately unsatisfactory due to numerous limitations that constrained both technical performance and pedagogical impact.</p>
</section>
<section id="project-overview" class="level2">
<h2 class="anchored" data-anchor-id="project-overview">Project Overview</h2>
<p>This report summarizes work from Belmont University’s <a href="https://github.com/drscotthawley/DLAIE">“Deep Learning and AI Ethics” course, PHY/DSC/BSA 4420</a>. This year, we replaced the traditional lesson-assignment loop with project-based learning: as a class, we built a text-conditioned latent flow matching generative model from scratch. This approach covered all the key topics from earlier iterations while adding new content on flow matching / rectified flow models. It also addressed post-ChatGPT educational challenges (where coding assignments are moot) and drove greater student engagement. The course included student-led lectures, a <a href="https://www.linkedin.com/feed/update/urn:li:activity:7386884786142937088/">Leaderboard Contest</a> with industry sponsors, and plans to publish a scholarly research paper—still in progress. Further details on the educational approach appear in section “Educational Approach”. We continue now with the technical aspects of guidance for flow models.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In generative AI, flow-based models have become a powerful technique that offers a simulation-free alternative to traditional diffusion-based approaches. At its core, flow matching involves learning a vector field that transports randomized samples to a complex target distribution along a predefined path. Unlike score-based diffusion models that rely on stochastic sampling and iterative denoising, flow matching directly regresses the velocity field needed to traverse this path, enabling more efficient and stable training. Prior research has adapted this framework to operate within learned latent representations, allowing for scalable generation across images and audio. Training-based guidance further enhances flow models by conditioning the learned velocity field on structured features. This research explores whether guidance-based flow-matching models can be applied to musical features like chords, melody, and beats to shape the output in latent space and improve audio quality.</p>
<p>Stable Audio Open is an open-source generative text-to-audio model developed by Stability AI that produces stereo audio at 44.1kHz from descriptive prompts. Since Stable Audio Open is already built on a flow-matching architecture, it provided an ideal foundation for integrating structured guidance. Using this as the baseline model and then incorporating plugin driven musical analysis to condition the latent velocity fields enables evaluation of whether embedding musical priors into the generative process leads to perceptually richer and more stylistically coherent audio outputs.</p>
<p>Moreover, this paper focuses on the importance of a hands-on project-based approach to AI education. Experiential learning, i.e.&nbsp;learning through direct experience, has become increasingly vital in AI education, especially as models grow more complex and abstract. Rather than relying solely on lectures or passive content consumption, such immersion fosters deeper conceptual understanding and retention.</p>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>This section provides foundational context for the techniques employed in this research. We first introduce flow models as a generative framework, then discuss how guidance mechanisms can be integrated to steer generation toward desired outputs. Understanding these concepts is essential for appreciating how musical features can be embedded into the latent generation process.</p>
<section id="what-is-a-flow-model" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-flow-model">What is a Flow Model?</h3>
<p>A flow model learns a continuous transformation that maps a simple noise distribution to a complex data distribution by following a differential equation over time. Flow matching provides a way to train these models by directly regressing the velocity field that moves a sample along a chosen path from noise to data. Because the method learns the true deterministic velocity rather than relying on stochastic denoising steps, it simplifies training and avoids the iterative noise-removal loops used in diffusion-based approaches.</p>
</section>
<section id="what-is-guidance" class="level3">
<h3 class="anchored" data-anchor-id="what-is-guidance">What is Guidance?</h3>
<p>Guidance methods can be used to add more precise controls to a flow model. In essence, guidance adjusts a generative model’s direction repeatedly during its generation towards some desired output. Guidance was originally used as a method for diffusion models, but recent results have shown that the same methods can be used to add controls to flow models. For flow models, guidance methods act on the velocity field, adding extra velocity at each step by evaluating the likelihood of reaching a desired condition at the current step. In classifier guidance, this desired condition is determined by the likelihood of matching an external classifier. The direction of the adjustment is found by calculating the gradient of the classifier log-likelihood.</p>
<p>PnP-Flow offers an alternative approach by adjusting the sample directly rather than modifying the velocity field. At each step, the method applies the gradient correction, projects back onto the flow trajectory (this is is easy to do with the assumption of the straightened paths from the ReFlow process), and then applies a denoiser, computing a new estimate. This cycle enables effective inference-time control even for models with nearly linear rectified paths. In practice, the approach allows external constraints to be incorporated while keeping the sample close to the model’s learned path, as demonstrated in recent work on plug-and-play flow matching.</p>
<p><img src="https://hackmd.io/_uploads/Bk8RLKuzbl.png" class="img-fluid" alt="guidance"> <em>This diagram illustrates how Δv corrects the predicted flow to reach the true target trajectory.</em></p>
</section>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<p>Our approach extends Stable Audio Open with guidance mechanisms targeting three core musical attributes: continuous pitch, discrete pitch, and beat. Each guidance type required developing or adapting differentiable analysis tools that could propagate gradients through the generation process. The following subsections detail the implementation of each guidance mechanism, the modifications required to existing libraries, and the integration strategy with the base model’s callback loop.</p>
<section id="continuous-pitch" class="level3">
<h3 class="anchored" data-anchor-id="continuous-pitch">Continuous Pitch</h3>
<p>The continuous pitch section of this paper explored finding a library to carry gradients through the pitch classification process on a continuous scale. While unsuccessful in finding a exact library to accomplish this goal, torchcrepe was an exact implementation of CREPE which allowed for the good representation of pitch estimation.</p>
<p>From this baseline, a fork of the torchcrepe library was taken and the following changes were made:</p>
<ol type="1">
<li>Removed <span class="citation" data-cites="with">@with</span> torch.nograd()</li>
<li>Replaced the viterbi decoder with softmax from torch.nn.functional</li>
<li>Replaced resampling process with torchaudio.functional.resample</li>
<li>Added a parameter to the library to control whether to use the gradient implementation.</li>
</ol>
<p>Using these changes to torchcrepe, they are implemented into the callback loop for stable audio open small. Using this implementation, the latents that are given at each epoch are decoded, classified by torchcrepe, and compared to the target pitch with MSE loss, using gradients to carry changes into the original flow model.</p>
</section>
<section id="discrete-pitch" class="level3">
<h3 class="anchored" data-anchor-id="discrete-pitch">Discrete Pitch</h3>
<p>The discrete pitch guidance approach focused on steering generation toward specific musical notes rather than continuous frequency values. Unlike continuous pitch estimation, discrete pitch classification maps audio to one of twelve semitone classes within an octave, providing a more musically meaningful representation for harmonic content.</p>
<p>For this implementation, we utilized a pretrained chord recognition model that outputs probability distributions over pitch classes. The model processes mel-spectrogram representations of the decoded audio at each generation step. To enable gradient flow, we replaced argmax operations with softmax temperature scaling, allowing the classification outputs to remain differentiable while still approximating discrete note selection.</p>
<p>The guidance signal is computed by comparing the predicted pitch class distribution against a target distribution representing the desired note or chord. Cross-entropy loss between these distributions provides the gradient signal that adjusts the latent velocity field. This approach aligns with recent work on posterior sampling in flow models, where likelihood gradients guide generation toward target conditions.</p>
</section>
<section id="beat" class="level3">
<h3 class="anchored" data-anchor-id="beat">Beat</h3>
<p>To guide the model toward a specific beat pattern, we incorporated volume loss into the sampling loop. At each diffusion step, we first decode the latent representation into audio. We then extract an onset-strength envelope, which measures changes in energy associated with note attacks and rhythmic transients. This creates a differentiable representation of the audio’s beat structure.</p>
<p>Before generation begins, we compute the onset envelope for the target reference audio using the same process. During sampling, we calculate the mean squared error between the generated and target onset envelopes, and backpropagate this loss into the latent representation. This encourages the model to shift its timing toward the target rhythm.</p>
</section>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<section id="general-limitations" class="level3">
<h3 class="anchored" data-anchor-id="general-limitations">General Limitations</h3>
<p>Colab limitations: We mainly operated in Google Colab which restricted the memory and processor access, reducing ability to quickly iterate and refine outcomes. GPU session timeouts and RAM constraints limited batch sizes and the number of generation steps we could feasibly evaluate.</p>
<p>For Mac users, a MPS limitation occurs with audio generation. Due to this error, Mac users will find their audio to be incorrectly generated.</p>
<ol type="1">
<li>The flow model’s impact on latents is overwhelmingly strong compared to the guidance we implement. Despite making several attempts to make guidance stronger, the model would reset many relevant changes towards trajectory.</li>
<li>Text conditioning needed to align with the guidance being implemented. For example, if we were attempting to guide a scale with the ChromaSpectrogram, while the prompt was asking for a drumset effect, guidance would fail.</li>
<li>Limited computational resources restricted our ability to train higher-capacity models or run long multi-epoch experiments. As a result, most guidance evaluations were performed on shortened training cycles, which may underrepresent the full potential of each method.</li>
<li>The small parameter count of Stable Audio Open Small limited the effectiveness of guidance. With fewer layers and reduced representational capacity, the model often prioritized maintaining its learned trajectory rather than adapting to external conditioning signals.</li>
</ol>
</section>
<section id="continuous-pitch-limitations" class="level3">
<h3 class="anchored" data-anchor-id="continuous-pitch-limitations">Continuous Pitch Limitations</h3>
<p>The continuous pitch section of this project faced many present challenges pertaining to effective generation of audio. While torchcrepe was effective in the implementation of a callback loop with stable audio open small, issues were present which are listed below.</p>
<ol type="1">
<li>Condensing ranges for the target pitch and the model pitch at each step, comparing them on a continuous scale lead to high loss. This high loss would have a difference that is too large for the model to handle.</li>
<li>Rewriting torchcrepe in a hasty fashion and switching out Viterbi for Softmax resulted in less accurate pitch classification.</li>
</ol>
</section>
<section id="discrete-pitch-limitations" class="level3">
<h3 class="anchored" data-anchor-id="discrete-pitch-limitations">Discrete Pitch Limitations</h3>
<ol type="1">
<li>The discretization of pitch space loses fine-grained intonation information, making the guidance less effective for microtonal or pitch-bent content.</li>
</ol>
</section>
<section id="beat-limitations" class="level3">
<h3 class="anchored" data-anchor-id="beat-limitations">Beat Limitations</h3>
<ol type="1">
<li><p>The system’s performance demonstrated limited rhythmic generalization because it fails to reliably generate outputs for rhythms outside the training data distribution, struggling with unusually slow or fast tempo and atypical temporal structures like silence at the start of the clip.</p></li>
<li><p>Usage of high strength volume guidance typically resulted in output audio with a different tempo than the target audio. Low guidance loss in the output audio was instead reached by an arrhythmic volume envelope.</p></li>
<li><p>A hierarchical conflict was observed where global tempo conditioning through text consistently supersedes the fine-grained rhythmic guidance, resulting in a loss of granular control when multiple conditional inputs are active.</p></li>
</ol>
</section>
</section>
<section id="an-educational-approach" class="level2">
<h2 class="anchored" data-anchor-id="an-educational-approach">An Educational Approach</h2>
<p>Littered throughout this post are references to the slightly fragmented and disjointed approach taken by fifteen students and an instructor with different educational backgrounds, existing knowledge, computational resources, and working environments. Although many might see this as a significant obstacle to teaching an undergraduate-level class, especially in a field that requires a hands-on approach to learning, these differences enriched the learning experience of each student by offering a concise foundation to Deep Learning, creating healthy competition between students, and providing a series of Jupyter notebooks which led students from basic MNIST classification to audio guidance. While the final goal of writing and submitting a paper proved to be a goal out of the reach of fifteen undergraduates (and one grad student), there were significant educational outcomes. Turning students from Deep Learning beginners and novices into intermediates in one semester is no simple feat, and the structure of this class – from its helpful learning resources to its exciting competitions – could serve as a framework for future classes.</p>
<section id="deep-learning-briefer" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-briefer">Deep Learning Briefer</h3>
<p>The class began with a speed run through the fundamental building blocks of Deep Learning. Layers, neurons, mulit-layer perceptrons, activation functions, gradient descent, data loaders, epochs, etc. – all the language and concepts that Machine Learning practitioners take for granted were covered at a blistering pace during the first two weeks of the class. For those with experience in Deep Learning, these two weeks were a refreshing recap; for those without any experience, the two weeks were more akin to a baptism by fire. While the speed at which the concepts were covered did not allow for a more methodical approach to the mathematical concepts behind gradient descent or the nuances between every single activation function, the briefer allowed students to quickly acquaint themselves with the essential building blocks of Deep Learning. This was part of a deliberate decision to push students out of the comfort zone students usually inhabit during undergrad and towards more exciting and innovative projects.</p>
</section>
<section id="getting-up-to-speed" class="level3">
<h3 class="anchored" data-anchor-id="getting-up-to-speed">Getting Up to Speed</h3>
<p>For students with different levels of Python experience and other languages, the early notebooks were all about balancing an understanding of machine learning concepts while building the Python skills needed to implement them. We began with the distinction between Machine Learning and Deep Learning, overtime moving from a conceptual understanding to direct implementation.</p>
<p>The course follows an evolving stack of libraries: beginning with FastAi to reduce complexity, and maturing towards PyTorch, with sparse implementations of Lightning to introduce new architectures. Moving from classic Machine Learning technieuqes to learning the beginnings of Convolutional Neural Networks (CNN), Residual skip connections, UNets, Auto Encoders/Variational Auto Encoders, Flow Models, then guidance.</p>
</section>
</section>
<section id="conclusion-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-discussion">Conclusion and Discussion</h2>
<p>Can implementing guidance in flow models meaningfully affect musical attributes? Our experiments showed that guidance signals—those derived from ChromaSpectrogram and CREPE-based pitch classification—can interact with the flow model in measurable ways. By analyzing how these signals push against the model’s inherent trajectory, we were able to observe the limits of rigidity in Stable Audio Open Small. We found partial success when using chroma-based discrete pitch guidance, with clear influence on harmonic structure, while continuous pitch guidance produced minimal effects due to higher loss and weaker gradient influence. Overall, guidance can shape musical attributes, but its impact is strongly constrained by model size and the strength of its underlying velocity field.</p>
<p>Beyond technical outcomes, our project highlighted the value of teaching guidance for educational impact. Implementing custom code into existing model architectures gave students a deeper understanding of how flow models are built and how they behave under external conditioning. Working with guidance directly helped develop an intuition for how latent gradient impacts interact with the model’s learned trajectory. Additionally, exploring different forms of fine-tuning and time scheduling showed how these choices can directly influence guidance effectiveness. Together, these experiences suggest that project-based learning is a uniquely effective tactic for understanding machine learning ideas, especially within flow-based systems.</p>
<p>Looking forward, there is room for growth. Implementing guidance on larger audio generative models would allow for a stronger influence on musical attributes and help overcome the rigidity we observed in smaller architectures. Future work can further explore ways to address the limitations of CREPE-based pitch classification, specifically the challenges of calculating loss within continuous systems. Developing methods that create greater impact between the guidance steps and experimenting with alternate guidance schedules may also shape outcomes more effectively. Working to map the latent space of models like Stable Audio Open Small teaches valuable skills in optimization and understanding architectural limitations, laying the foundation for growth in both students and future guidance techniques.</p>
</section>
<section id="poster-presented-at-belmont-universitys-surs-2025" class="level2">
<h2 class="anchored" data-anchor-id="poster-presented-at-belmont-universitys-surs-2025">Poster Presented at Belmont University’s SURS 2025</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackmd.io/_uploads/B1QNBt_GZe.png" class="img-fluid figure-img"></p>
<figcaption>Screenshot 2025-12-11 at 11.53.34 AM</figcaption>
</figure>
</div>
</section>
<section id="acknowledgement" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgement">Acknowledgement</h2>
<p>Your mom</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/MaxGoskie\.github\.io\/dlaie_audio_showcase\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>